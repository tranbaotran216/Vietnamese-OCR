{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9794055,"sourceType":"datasetVersion","datasetId":6001787}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo\n!pip install fastwer","metadata":{"id":"48_XF-iTiaJE","outputId":"6604277c-e94e-4b0d-fa6f-455d8bebaf8c","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:45:49.304844Z","iopub.execute_input":"2025-06-23T14:45:49.305017Z","iopub.status.idle":"2025-06-23T14:46:07.579864Z","shell.execute_reply.started":"2025-06-23T14:45:49.305001Z","shell.execute_reply":"2025-06-23T14:46:07.579152Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\nCollecting fastwer\n  Downloading fastwer-0.1.3.tar.gz (4.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (from fastwer) (2.13.6)\nBuilding wheels for collected packages: fastwer\n  Building wheel for fastwer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fastwer: filename=fastwer-0.1.3-cp311-cp311-linux_x86_64.whl size=866377 sha256=8614eaf236e2c16bc68674b7ea754ce1ecdd442f7e28b258dc915795cb566b67\n  Stored in directory: /root/.cache/pip/wheels/4c/53/1e/8d806da8c1ed1de60e371005658af32b92aad9426b37208f1f\nSuccessfully built fastwer\nInstalling collected packages: fastwer\nSuccessfully installed fastwer-0.1.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport os\nimport fastwer\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"3f515313","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:07.581104Z","iopub.execute_input":"2025-06-23T14:46:07.581436Z","iopub.status.idle":"2025-06-23T14:46:11.953923Z","shell.execute_reply.started":"2025-06-23T14:46:07.581410Z","shell.execute_reply":"2025-06-23T14:46:11.953393Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.listdir('/kaggle/input/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:11.955306Z","iopub.execute_input":"2025-06-23T14:46:11.955608Z","iopub.status.idle":"2025-06-23T14:46:11.969973Z","shell.execute_reply.started":"2025-06-23T14:46:11.955592Z","shell.execute_reply":"2025-06-23T14:46:11.969414Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['Vietnam']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"val_word_data = pd.read_csv('/kaggle/input/Vietnam/validation_word.csv', sep='\\t', index_col=0)\ntrain_word_data = pd.read_csv('/kaggle/input/Vietnam/train_word.csv', sep='\\t', index_col=0)\ntest_word_data = pd.read_csv('/kaggle/input/Vietnam/test_word.csv', sep='\\t', index_col=0)","metadata":{"id":"22905c80","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:11.970589Z","iopub.execute_input":"2025-06-23T14:46:11.970784Z","iopub.status.idle":"2025-06-23T14:46:12.241851Z","shell.execute_reply.started":"2025-06-23T14:46:11.970769Z","shell.execute_reply":"2025-06-23T14:46:12.241263Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_word_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.242531Z","iopub.execute_input":"2025-06-23T14:46:12.242722Z","iopub.status.idle":"2025-06-23T14:46:12.259814Z","shell.execute_reply.started":"2025-06-23T14:46:12.242705Z","shell.execute_reply":"2025-06-23T14:46:12.259054Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                             id  label\n0  20140603_0003_BCCTC_tg_0_0_0    Bản\n1  20140603_0003_BCCTC_tg_0_0_1   chất\n2  20140603_0003_BCCTC_tg_0_0_2    của\n3  20140603_0003_BCCTC_tg_0_0_3  thành\n4  20140603_0003_BCCTC_tg_0_0_4   công","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20140603_0003_BCCTC_tg_0_0_0</td>\n      <td>Bản</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20140603_0003_BCCTC_tg_0_0_1</td>\n      <td>chất</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20140603_0003_BCCTC_tg_0_0_2</td>\n      <td>của</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20140603_0003_BCCTC_tg_0_0_3</td>\n      <td>thành</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20140603_0003_BCCTC_tg_0_0_4</td>\n      <td>công</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class Vocab():\n    def __init__(self, chars):\n        self.pad = 0\n        self.go = 1\n        self.eos = 2\n        self.mask_token = 3\n\n        self.chars = chars\n\n        self.c2i = {c:i+4 for i, c in enumerate(chars)}\n\n        self.i2c = {i+4:c for i, c in enumerate(chars)}\n\n        self.i2c[0] = '<pad>'\n        self.i2c[1] = '<sos>'\n        self.i2c[2] = '<eos>'\n        self.i2c[3] = '*'\n\n    def encode(self, chars):\n        return [self.go] + [self.c2i[c] for c in chars] + [self.eos]\n\n    def decode(self, ids):\n        if isinstance(ids, np.ndarray):\n          ids = ids.tolist()\n        filtered_ids = [i for i in ids if i > 3 and i in self.i2c]\n        \n        if self.eos in ids:\n            eos_index = ids.index(self.eos)\n            filtered_ids = [i for i in ids[:eos_index] if i > 3 and i in self.i2c]\n\n        sent = ''.join([self.i2c[i] for i in filtered_ids])\n        return sent\n\n    def __len__(self):\n        return len(self.c2i) + 4\n\n    def batch_decode(self, arr):\n        texts = [self.decode(ids) for ids in arr]\n        return texts\n\n    def __str__(self):\n        return self.chars","metadata":{"id":"d7e1d15b","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.260642Z","iopub.execute_input":"2025-06-23T14:46:12.260903Z","iopub.status.idle":"2025-06-23T14:46:12.268232Z","shell.execute_reply.started":"2025-06-23T14:46:12.260881Z","shell.execute_reply":"2025-06-23T14:46:12.267610Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Tạp kí tự\nvocab = Vocab('aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ ')\n# char2idx = {char : idx for idx, char in enumerate(vocab)}\n# idx2char = {idx: char for idx, char in enumerate(vocab)}\nvocab_size = vocab.__len__()\nvocab.__str__()","metadata":{"id":"05978a1d","outputId":"690eaae1-f777-425c-9bf6-7098dbeda6fc","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.268921Z","iopub.execute_input":"2025-06-23T14:46:12.269216Z","iopub.status.idle":"2025-06-23T14:46:12.282128Z","shell.execute_reply.started":"2025-06-23T14:46:12.269168Z","shell.execute_reply":"2025-06-23T14:46:12.281465Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## **preprocessing image**","metadata":{"id":"34a36756"}},{"cell_type":"code","source":"def process(img_path):\n    img = cv2.imread(img_path, 0) \n    if img is None:\n        return None\n    kernel = np.ones((5, 5), np.uint8)\n \n    image = cv2.GaussianBlur(img, (5, 5), 0)\n    \n \n    image = cv2.adaptiveThreshold(\n        image, 255, \n        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        cv2.THRESH_BINARY,\n        11, 5\n    )\n \n    kernel = np.ones((11, 11), np.uint8)\n    img_erosion = cv2.erode(image, kernel, iterations=1)\n    \n    return img_erosion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.282884Z","iopub.execute_input":"2025-06-23T14:46:12.283234Z","iopub.status.idle":"2025-06-23T14:46:12.294965Z","shell.execute_reply.started":"2025-06-23T14:46:12.283188Z","shell.execute_reply":"2025-06-23T14:46:12.294339Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"widths= []\nheights = []\ndef preprocess_image(image_path, label_len, img_h=64, char_w=32):\n    valid_img = True\n    img = process(image_path)\n\n    try:\n        cur_h, cur_w = img.shape\n        modified_w = int(cur_w * (img_h / cur_h))\n\n        # Kiểm tra tỷ lệ hợp lệ giữa độ rộng ảnh và độ dài nhãn\n        if ((modified_w / label_len) < (char_w / 4)) or ((modified_w / label_len) > (4 * char_w)):\n            valid_img = False\n        else:\n            img_w = label_len * char_w\n            img = cv2.resize(img, (img_w, img_h), interpolation=cv2.INTER_AREA)\n\n    except AttributeError:\n        valid_img = False\n\n    if not valid_img:\n        return None\n\n    # Thêm chiều kênh (từ HxW → 1xHxW)\n    img = np.expand_dims(img, axis=0)\n\n    # Chuẩn hóa giá trị pixel về [0,1]\n    img = img / 255.0\n\n    return torch.tensor(img, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.297410Z","iopub.execute_input":"2025-06-23T14:46:12.297712Z","iopub.status.idle":"2025-06-23T14:46:12.306593Z","shell.execute_reply.started":"2025-06-23T14:46:12.297686Z","shell.execute_reply":"2025-06-23T14:46:12.306051Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class HandwrittenDataset(Dataset):\n    def __init__(self, data, image_dir, vocab, data_type='generic'):\n        # loại bỏ các dòng trong data có label rỗng/ NaN\n        self.data = data.dropna(subset=['label'])\n        self.data = self.data[self.data['label'].str.strip() != '']\n        self.image_dir = image_dir\n        self.vocab = vocab\n        self.data_type = data_type\n\n    def __len__(self):\n        return len(self.data) # số lượng sample trong data\n\n    def clean_text(self, text):\n        # ký tự ko có trong vocab thì bỏ\n        allowed_chars = set(self.vocab.c2i.keys())  # Set các ký tự hợp lệ từ vocab\n        return ''.join([char if char in allowed_chars else '' for char in text])  # Loại bỏ ký tự không có trong vocab\n\n    def encode_to_labels(self, label, max_length=None):\n        label = self.clean_text(label)  # bỏ mấy kí tự ko trong vocab\n        label_indices = self.vocab.encode(label)\n\n        if max_length is not None and len(label_indices) < max_length:\n            label_indices = label_indices + [self.vocab.pad] * (max_length - len(label_indices))\n\n        return label_indices # kí tự sang chuỗi số (list[int])\n\n    def __getitem__(self, idx): # lấy img và label tương ứng idx\n\n        img_id = self.data.iloc[idx]['id']\n        label = self.data.iloc[idx]['label']\n\n        label_len = len(label)\n    \n        image_path = os.path.join(self.image_dir, f\"{img_id}.png\")\n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"Không tìm thấy tệp: {image_path}\")\n\n        image = preprocess_image(image_path, label_len)\n        if image == None:\n            return self.__getitem__((idx + 1) % len(self))\n        else:\n            label_indices = self.encode_to_labels(label, max_length=None)\n            label_tensor = torch.tensor(label_indices, dtype=torch.long)\n    \n            label_length = len(label_indices) # len nhưng mà là len của tensor\n    \n            # length: có padding\n            # len: ko có\n            return image, label_tensor, label_length, label_len # ký tự nhãn -> tensor\n\n    def show_samples(self, idx=None): #in mẫu coi chơi\n        plt.figure(figsize=(15, 5))\n\n        if idx is None:\n            idx = np.random.randint(0, len(self))\n\n        image, label_tensor,_, label_len = self[idx]\n\n        image = image.squeeze(0).numpy()\n        image = (image * 255).astype(np.uint8)\n\n        decoded_label = self.vocab.decode(label_tensor.tolist())\n        print(f\"Label_length: {label_len}\")\n\n        print(f\"Encoded Label: {label_tensor}\")\n        print(f\"Decoded Label: {decoded_label}\")\n        print(f\"Image Shape: {image.shape}\")\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n        plt.show()\n\n","metadata":{"id":"9d0f6347","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.307259Z","iopub.execute_input":"2025-06-23T14:46:12.307429Z","iopub.status.idle":"2025-06-23T14:46:12.318013Z","shell.execute_reply.started":"2025-06-23T14:46:12.307416Z","shell.execute_reply":"2025-06-23T14:46:12.317465Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def handwritten_collate_fn(batch):\n    # các ảnh trong batch dạng tensor\n    # label của mấy ảnh đó cx dạng tensor\n    images, labels, label_lengths, label_lens = zip(*batch)\n\n    # chìu dài tối đa của toàn bộ labels trong batch (tính trên số kí tự)\n\n    #chìu rộng lớn của toàn bộ ảnh trogn 1 batch\n    max_width = max(img.shape[2] for img in images)\n    \n    # padding ảnh về cùng width\n    padded_images = []\n    for img in images:\n        _, h, w = img.shape\n        if w < max_width:\n            pad_size = max_width - w\n            padding = torch.full((1, h, pad_size), fill_value=1.0)  # Padding bằng 1 (giả sử nền trắng)\n            padded_img = torch.cat([img, padding], dim=2)\n        else:\n            padded_img = img\n        padded_images.append(padded_img)\n        \n    images_tensor = torch.stack(padded_images)\n\n    max_label_length = max(label_lengths)\n    # padding cho mấy cái label có cùng độ dài\n    labels_padded = torch.zeros(len(labels), max_label_length, dtype=torch.long)\n    for i, label in enumerate(labels):\n        labels_padded[i, :label_lengths[i]] = label\n\n    label_lengths_tensor = torch.tensor(label_lengths, dtype=torch.long)\n\n    return images_tensor, labels_padded, label_lengths_tensor","metadata":{"id":"c830530c","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.318742Z","iopub.execute_input":"2025-06-23T14:46:12.319029Z","iopub.status.idle":"2025-06-23T14:46:12.334798Z","shell.execute_reply.started":"2025-06-23T14:46:12.319007Z","shell.execute_reply":"2025-06-23T14:46:12.334246Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Tạo dataset cho từng bộ dữ liệu\nword_train_dataset = HandwrittenDataset(train_word_data, '/kaggle/input/Vietnam/train_word', vocab)\nword_val_dataset = HandwrittenDataset(val_word_data, '/kaggle/input/Vietnam/validation_word', vocab)\nword_test_dataset = HandwrittenDataset(test_word_data, '/kaggle/input/Vietnam/test_word', vocab)","metadata":{"id":"3b251be3","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.335409Z","iopub.execute_input":"2025-06-23T14:46:12.335601Z","iopub.status.idle":"2025-06-23T14:46:12.394453Z","shell.execute_reply.started":"2025-06-23T14:46:12.335588Z","shell.execute_reply":"2025-06-23T14:46:12.393942Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"word_train_dataset.show_samples(6)\nword_train_dataset.show_samples(7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.395096Z","iopub.execute_input":"2025-06-23T14:46:12.395316Z","iopub.status.idle":"2025-06-23T14:46:12.750930Z","shell.execute_reply.started":"2025-06-23T14:46:12.395299Z","shell.execute_reply":"2025-06-23T14:46:12.750230Z"}},"outputs":[{"name":"stdout","text":"Label_length: 3\nEncoded Label: tensor([  1,  40,   4, 100,   2])\nDecoded Label: bao\nImage Shape: (64, 96)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Label_length: 3\nEncoded Label: tensor([  1,  74,  78, 126,   2])\nDecoded Label: giờ\nImage Shape: (64, 96)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_loader = DataLoader(word_train_dataset, batch_size=16, shuffle=True, collate_fn=handwritten_collate_fn, num_workers=4, pin_memory=True)\nval_loader = DataLoader(word_val_dataset, batch_size=16, shuffle=True, collate_fn=handwritten_collate_fn, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(word_test_dataset, batch_size=16, shuffle=True, collate_fn=handwritten_collate_fn, num_workers=4, pin_memory=True)","metadata":{"id":"2d0de411","outputId":"1a264b82-b0d2-4255-cfb6-cdfa77f69c8d","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.751812Z","iopub.execute_input":"2025-06-23T14:46:12.752105Z","iopub.status.idle":"2025-06-23T14:46:12.756864Z","shell.execute_reply.started":"2025-06-23T14:46:12.752088Z","shell.execute_reply":"2025-06-23T14:46:12.756118Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#Lấy một batch từ DataLoader\ni = 0\nfor images, labels_padded, label_lengths in train_loader:\n    i += 1\n    if i == 3:\n        print(\"Images Shape:\", images.shape)  # (batch_size, 1, 256, 512)\n        print(\"Labels Padded Shape:\", labels_padded.shape)  # (batch_size, max_label_length)\n        print(\"Label Lengths:\", label_lengths)  # (batch_size,)\n    \n        # Lấy ảnh đầu tiên từ batch\n        first_image = images[3].squeeze(0).numpy()  # Loại bỏ kênh chiều 1\n    \n        # Hiển thị ảnh\n        plt.imshow(first_image, cmap='gray')\n        plt.title(f\"Label: {labels_padded[0][:label_lengths[0]].tolist()}\")\n        plt.axis('off')  # Tắt các trục\n        plt.show()\n    \n        break\n","metadata":{"id":"606382fc","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:12.757642Z","iopub.execute_input":"2025-06-23T14:46:12.757908Z","iopub.status.idle":"2025-06-23T14:46:13.702687Z","shell.execute_reply.started":"2025-06-23T14:46:12.757891Z","shell.execute_reply":"2025-06-23T14:46:13.701799Z"}},"outputs":[{"name":"stdout","text":"Images Shape: torch.Size([16, 1, 64, 128])\nLabels Padded Shape: torch.Size([16, 6])\nLabel Lengths: tensor([5, 4, 5, 6, 6, 4, 4, 6, 5, 5, 5, 6, 6, 4, 5, 6])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# # Kiểm tra từng bộ dữ liệu bằng cách hiển thị một số mẫu\n# print(\"Line Dataset Sample:\")\n# line_dataset.show_samples(0)\n\n# print(\"Word Dataset Sample:\")\n# word_dataset.show_samples(0)\n\n# print(\"Paragraph Dataset Sample:\")\n# para_dataset.show_samples(0)","metadata":{"id":"e4738987","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.703873Z","iopub.execute_input":"2025-06-23T14:46:13.704697Z","iopub.status.idle":"2025-06-23T14:46:13.708023Z","shell.execute_reply.started":"2025-06-23T14:46:13.704670Z","shell.execute_reply":"2025-06-23T14:46:13.707171Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## **Calculate CER and WER**","metadata":{"id":"d5wcYU9M1jE7"}},{"cell_type":"code","source":"import editdistance","metadata":{"id":"VXSMR0DY1-Ez","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.708757Z","iopub.execute_input":"2025-06-23T14:46:13.708930Z","iopub.status.idle":"2025-06-23T14:46:13.727082Z","shell.execute_reply.started":"2025-06-23T14:46:13.708917Z","shell.execute_reply":"2025-06-23T14:46:13.726521Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def cal_cer(predicted, target):\n  predicted_text = predicted.replace(' ','')\n  target_text = target.replace(' ','')\n  cer = editdistance.eval(predicted_text, target_text) / float(max(len(predicted_text), len(target_text)))\n  return cer","metadata":{"id":"_rr2afVX1oKt","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.727914Z","iopub.execute_input":"2025-06-23T14:46:13.728169Z","iopub.status.idle":"2025-06-23T14:46:13.734486Z","shell.execute_reply.started":"2025-06-23T14:46:13.728145Z","shell.execute_reply":"2025-06-23T14:46:13.733889Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def cal_wer(predicted, target):\n  predicted_words = predicted.split()\n  target_words = target.split()\n  wer = editdistance.eval(predicted_words, target_words) / float(max(len(predicted_words), len(target_words)))\n  return wer","metadata":{"id":"DhUbgEdU2KQR","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.735168Z","iopub.execute_input":"2025-06-23T14:46:13.735421Z","iopub.status.idle":"2025-06-23T14:46:13.747175Z","shell.execute_reply.started":"2025-06-23T14:46:13.735400Z","shell.execute_reply":"2025-06-23T14:46:13.746530Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## **Eval function**","metadata":{"id":"DsGUq3DK3i9P"}},{"cell_type":"code","source":"def evaluate(model, dataloader, vocab, device):\n    model.eval()\n    total_cer = 0\n    total_wer = 0\n    total_correct_chars = 0  # tổng số ký tự đúng\n    total_chars = 0          # tổng số ký tự thực\n    sample_count = 0\n\n    with torch.no_grad():\n        for images, labels_padded, label_lengths in dataloader:\n            images = images.to(device)\n            labels_padded = labels_padded.to(device)\n\n            # Forward\n            outputs = model(images)  # (seq_len, batch, vocab_size)\n            log_probs = F.log_softmax(outputs, dim=2)\n\n            predicted_indices = log_probs.argmax(dim=2)  # (seq_len, batch)\n            predicted_texts = vocab.batch_decode(predicted_indices.T.cpu().numpy())\n\n            labels_padded_np = labels_padded.cpu().numpy()\n            label_lengths_np = label_lengths.cpu().numpy()\n\n            # Giải mã nhãn thật\n            target_texts = []\n            for i in range(len(labels_padded_np)):\n                unpadded = labels_padded_np[i][:label_lengths_np[i]]\n                decoded = vocab.decode(unpadded)\n                target_texts.append(decoded)\n\n            for i, (pred, target) in enumerate(zip(predicted_texts, target_texts)):\n                if not pred or not target: \n                    continue\n                pred = pred.replace('*', '')\n\n                if sample_count < 5:\n                    print(f\"Predicted: {repr(pred)}\")\n                    print(f\"Target:    {repr(target)}\")\n\n                sample_count += 1\n                    \n\n                total_cer += fastwer.score_sent(pred, target, char_level=True)/100\n                total_wer += fastwer.score_sent(pred, target, char_level=False)/100\n\n                # Tính accuracy theo ký tự\n                min_len = min(len(pred), len(target))\n                correct = sum(1 for a, b in zip(pred[:min_len], target[:min_len]) if a == b)\n                total_correct_chars += correct\n                total_chars += len(target)\n\n    avg_cer = total_cer / sample_count\n    avg_wer = total_wer / sample_count\n    avg_char_acc = total_correct_chars / total_chars if total_chars > 0 else 0.0\n\n    return avg_cer, avg_wer, avg_char_acc\n","metadata":{"id":"x4VmnjDM3ioA","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.747916Z","iopub.execute_input":"2025-06-23T14:46:13.748116Z","iopub.status.idle":"2025-06-23T14:46:13.757252Z","shell.execute_reply.started":"2025-06-23T14:46:13.748102Z","shell.execute_reply":"2025-06-23T14:46:13.756725Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## **Build model**","metadata":{"id":"c2f1a33a"}},{"cell_type":"markdown","source":"### 5l cnn, 3l lstm, linear","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass CRNN_LSTM_Lite(nn.Module):\n    def __init__(self, vocab_size, img_channels=1, lstm_hidden=256):\n        super(CRNN_LSTM_Lite, self).__init__()\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(img_channels, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2), # h/2\n            nn.Dropout(0.1),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2), #h/4\n            nn.Dropout(0.1),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2), #h/8\n            nn.Dropout(0.1),\n\n            nn.Conv2d(256, 256, kernel_size=3, stride=(2,2), padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(), \n            nn.Dropout(0.1), #h/16\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=(2,1), padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Dropout(0.1),#h/32\n\n            nn.Conv2d(512, 512, kernel_size=3,  stride=(2,1), padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2) #h/64\n        )\n\n        # Tính kích thước đầu vào LSTM từ chiều cao ảnh đầu vào (ví dụ: H=64)\n        self.feature_h = 64 // 64  # chiều cao còn lại sau pooling\n\n        self.lstm = nn.LSTM(\n            input_size=512 * self.feature_h,\n            hidden_size=lstm_hidden,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True\n        )\n\n        self.dropout = nn.Dropout(0.3)\n\n        self.classifier = nn.Linear(lstm_hidden *2 , vocab_size)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        features = self.cnn(x)  # (B, 512, H/8, W/4)\n\n        # (B, C, H, W) -> (B, W, C*H)\n        b, c, h, w = features.size()\n        features = features.permute(0, 3, 1, 2)  # (B, W, C, H)\n        features = features.contiguous().view(b, w, c * h)  # (B, W, C*H)\n\n        lstm_out = self.lstm(features)[0]  # (B, W, 2*hidden)\n        lstm_out = self.dropout(lstm_out)\n        output = self.classifier(lstm_out)  # (B, W, vocab_size)\n        output = output.permute(1, 0, 2)  # (seq_len, batch, vocab_size) cho CTC Loss\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.758076Z","iopub.execute_input":"2025-06-23T14:46:13.758663Z","iopub.status.idle":"2025-06-23T14:46:13.777303Z","shell.execute_reply.started":"2025-06-23T14:46:13.758631Z","shell.execute_reply":"2025-06-23T14:46:13.776631Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class ResidualBlock(nn.Module):\n#     def __init__(self, channels):\n#         super().__init__()\n#         self.conv = nn.Sequential(\n#             nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(channels),\n#             nn.ReLU(),\n#             nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(channels)\n#         )\n#         self.relu = nn.ReLU()\n\n#     def forward(self, x):\n#         return self.relu(self.conv(x) + x)\n\n# class CRNN_LSTM_Improved(nn.Module):\n#     def __init__(self, vocab_size, img_channels=1, lstm_hidden=256):\n#         super().__init__()\n\n#         self.cnn = nn.Sequential(\n#             nn.Conv2d(img_channels, 64, kernel_size=3, padding=1),  # (B, 64, 64, W)\n#             nn.BatchNorm2d(64),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2,2),  # -> (B, 64, 32, W/2)\n#             nn.Dropout(0.1),\n\n#             nn.Conv2d(64, 128, kernel_size=3, padding=1),           # -> (B, 128, 32, W/2)\n#             nn.BatchNorm2d(128),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2,2),                                      # -> (B, 128, 16, W/4)\n#             nn.Dropout(0.1),\n\n#             nn.Conv2d(128, 256, kernel_size=3, padding=1),          # -> (B, 256, 16, W/4)\n#             nn.BatchNorm2d(256),\n#             nn.ReLU(),\n#             ResidualBlock(256),                                     # Residual connection\n#             nn.Dropout(0.1),\n\n#             nn.Conv2d(256, 256, kernel_size=3, stride=(2,1), padding=1),  # -> (B, 256, 8, W/4)\n#             nn.BatchNorm2d(256),\n#             nn.ReLU(),\n#             nn.Dropout(0.1),\n\n#             nn.Conv2d(256, 512, kernel_size=3, padding=1),          # -> (B, 512, 8, W/4)\n#             nn.BatchNorm2d(512),\n#             nn.ReLU(),\n#             nn.AdaptiveAvgPool2d((8, None)),  # fix H = 8 regardless of input\n#             nn.Dropout(0.2)\n#         )\n\n#         self.feature_h = 8\n#         self.ln = nn.LayerNorm(512 * self.feature_h)\n\n#         self.lstm = nn.LSTM(\n#             input_size=512 * self.feature_h,\n#             hidden_size=lstm_hidden,\n#             num_layers=2,\n#             bidirectional=True,\n#             batch_first=True\n#         )\n\n#         self.dropout = nn.Dropout(0.3)\n#         self.classifier = nn.Linear(lstm_hidden * 2, vocab_size)\n\n#     def forward(self, x):\n#         features = self.cnn(x)  # (B, 512, 8, W')\n#         b, c, h, w = features.size()\n#         features = features.permute(0, 3, 1, 2).contiguous().view(b, w, c * h)  # (B, W', 512*8)\n\n#         features = self.ln(features)\n#         lstm_out, _ = self.lstm(features)  # (B, W', 2*lstm_hidden)\n#         lstm_out = self.dropout(lstm_out)\n#         output = self.classifier(lstm_out)  # (B, W', vocab_size)\n#         return output.permute(1, 0, 2)  # (seq_len, batch, vocab_size) for CTC\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.778037Z","iopub.execute_input":"2025-06-23T14:46:13.778304Z","iopub.status.idle":"2025-06-23T14:46:13.790654Z","shell.execute_reply.started":"2025-06-23T14:46:13.778288Z","shell.execute_reply":"2025-06-23T14:46:13.790119Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model = CRNN_LSTM_Lite(vocab_size=len(vocab))\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Tổng số tham số: {total_params:,}\")\nprint(f\"Số tham số có thể huấn luyện: {trainable_params:,}\")\n","metadata":{"id":"4sLlruNCGCao","outputId":"e8f188a7-ca4d-43e3-f94b-ad0ff9d420aa","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.791326Z","iopub.execute_input":"2025-06-23T14:46:13.791546Z","iopub.status.idle":"2025-06-23T14:46:13.876291Z","shell.execute_reply.started":"2025-06-23T14:46:13.791531Z","shell.execute_reply":"2025-06-23T14:46:13.875519Z"}},"outputs":[{"name":"stdout","text":"Tổng số tham số: 7,776,617\nSố tham số có thể huấn luyện: 7,776,617\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model, input_size=(1, 1, 64, 128))","metadata":{"id":"Rqy4-O8m0qUP","outputId":"cd7429f5-4682-456a-a37c-86c08e018ee0","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:13.877130Z","iopub.execute_input":"2025-06-23T14:46:13.877354Z","iopub.status.idle":"2025-06-23T14:46:14.576353Z","shell.execute_reply.started":"2025-06-23T14:46:13.877338Z","shell.execute_reply":"2025-06-23T14:46:14.575657Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCRNN_LSTM_Lite                           [8, 1, 233]               --\n├─Sequential: 1-1                        [1, 512, 1, 8]            --\n│    └─Conv2d: 2-1                       [1, 64, 64, 128]          640\n│    └─BatchNorm2d: 2-2                  [1, 64, 64, 128]          128\n│    └─ReLU: 2-3                         [1, 64, 64, 128]          --\n│    └─MaxPool2d: 2-4                    [1, 64, 32, 64]           --\n│    └─Dropout: 2-5                      [1, 64, 32, 64]           --\n│    └─Conv2d: 2-6                       [1, 128, 32, 64]          73,856\n│    └─BatchNorm2d: 2-7                  [1, 128, 32, 64]          256\n│    └─ReLU: 2-8                         [1, 128, 32, 64]          --\n│    └─MaxPool2d: 2-9                    [1, 128, 16, 32]          --\n│    └─Dropout: 2-10                     [1, 128, 16, 32]          --\n│    └─Conv2d: 2-11                      [1, 256, 16, 32]          295,168\n│    └─BatchNorm2d: 2-12                 [1, 256, 16, 32]          512\n│    └─ReLU: 2-13                        [1, 256, 16, 32]          --\n│    └─MaxPool2d: 2-14                   [1, 256, 8, 16]           --\n│    └─Dropout: 2-15                     [1, 256, 8, 16]           --\n│    └─Conv2d: 2-16                      [1, 256, 4, 8]            590,080\n│    └─BatchNorm2d: 2-17                 [1, 256, 4, 8]            512\n│    └─ReLU: 2-18                        [1, 256, 4, 8]            --\n│    └─Dropout: 2-19                     [1, 256, 4, 8]            --\n│    └─Conv2d: 2-20                      [1, 512, 2, 8]            1,180,160\n│    └─BatchNorm2d: 2-21                 [1, 512, 2, 8]            1,024\n│    └─ReLU: 2-22                        [1, 512, 2, 8]            --\n│    └─Dropout: 2-23                     [1, 512, 2, 8]            --\n│    └─Conv2d: 2-24                      [1, 512, 1, 8]            2,359,808\n│    └─BatchNorm2d: 2-25                 [1, 512, 1, 8]            1,024\n│    └─ReLU: 2-26                        [1, 512, 1, 8]            --\n│    └─Dropout: 2-27                     [1, 512, 1, 8]            --\n├─LSTM: 1-2                              [1, 8, 512]               3,153,920\n├─Dropout: 1-3                           [1, 8, 512]               --\n├─Linear: 1-4                            [1, 8, 233]               119,529\n==========================================================================================\nTotal params: 7,776,617\nTrainable params: 7,776,617\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 389.62\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 15.06\nParams size (MB): 31.11\nEstimated Total Size (MB): 46.19\n=========================================================================================="},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"### **DÙng early_stopping**","metadata":{"id":"ZSuCZvfPSTUY"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nctc_loss_fn = nn.CTCLoss(blank=vocab.mask_token, zero_infinity=True)\n\n# --- EarlyStopping class ---\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False, path='best_model.pth'):\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.verbose = verbose\n        self.path = path\n\n    def __call__(self, acc, model):\n        score = acc\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(model)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(model)\n            self.counter = 0\n\n    def save_checkpoint(self, model):\n        torch.save(model.state_dict(), self.path)\n        if self.verbose:\n            print(f\"Model improved. Saving model to {self.path}\")","metadata":{"id":"vp-PzxLjSX-o","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:14.577195Z","iopub.execute_input":"2025-06-23T14:46:14.577495Z","iopub.status.idle":"2025-06-23T14:46:14.584111Z","shell.execute_reply.started":"2025-06-23T14:46:14.577478Z","shell.execute_reply":"2025-06-23T14:46:14.583500Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n\n    for images, labels_padded, label_lengths in dataloader:\n        images = images.to(device)\n        labels_padded = labels_padded.to(device)\n\n        outputs = model(images)\n        log_probs = F.log_softmax(outputs, dim=2)\n\n        input_lengths = torch.full(\n            size=(images.size(0),),\n            fill_value=log_probs.size(0),\n            dtype=torch.long\n        ).to(device)\n\n        target_lengths = label_lengths.to(device)\n        targets = []\n        for i in range(len(labels_padded)):\n            targets.extend(labels_padded[i][:target_lengths[i]].tolist())\n        targets = torch.tensor(targets, dtype=torch.long).to(device)\n\n        loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)","metadata":{"id":"De4IRlETSgzp","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:14.585125Z","iopub.execute_input":"2025-06-23T14:46:14.585415Z","iopub.status.idle":"2025-06-23T14:46:14.602439Z","shell.execute_reply.started":"2025-06-23T14:46:14.585398Z","shell.execute_reply":"2025-06-23T14:46:14.601712Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CRNN_LSTM_Lite(vocab_size=len(vocab)).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nEPOCHS = 25\nearly_stopping = EarlyStopping(patience=5, verbose=True, path='best_model.pth')\n\ntrain_losses = []\nval_cers = []\nval_wers = []\nval_accs = []\n\nfor epoch in range(EPOCHS):\n    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n    train_losses.append(train_loss)\n\n    avg_cer, avg_wer, avg_acc = evaluate(model, val_loader, vocab, device)\n    val_cers.append(avg_cer)\n    val_wers.append(avg_wer)\n    val_accs.append(avg_acc)\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}\")\n    print(f\"Validation CER: {avg_cer * 100:.2f}%, WER: {avg_wer * 100:.2f}%, ACC: {avg_acc * 100:.2f}%\")\n\n    early_stopping(avg_acc, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered.\")\n        break","metadata":{"id":"6yXxt-O5xqHA","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:46:14.606161Z","iopub.execute_input":"2025-06-23T14:46:14.606583Z","execution_failed":"2025-06-23T15:57:51.401Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Predicted: '3'\nTarget:    '9'\nPredicted: 'nước'\nTarget:    'nước'\nPredicted: 'hác'\nTarget:    'bán'\nPredicted: 'người'\nTarget:    'người'\nPredicted: 'đồ'\nTarget:    'từ'\nEpoch 1/25, Train Loss: 0.8640\nValidation CER: 28.49%, WER: 55.87%, ACC: 80.08%\nModel improved. Saving model to best_model.pth\nPredicted: '091'\nTarget:    '091'\nPredicted: 'thi'\nTarget:    'thị'\nPredicted: 'quốc'\nTarget:    'quốc'\nPredicted: 'cái'\nTarget:    'cái'\nPredicted: 'sống'\nTarget:    'sống'\nEpoch 2/25, Train Loss: 0.3070\nValidation CER: 14.63%, WER: 37.60%, ACC: 87.41%\nModel improved. Saving model to best_model.pth\nPredicted: 'thiếu'\nTarget:    'thiếu'\nPredicted: 'Toàn'\nTarget:    'Toàn'\nPredicted: 'lên'\nTarget:    'lên'\nPredicted: 'ngồi'\nTarget:    'ngôi'\nPredicted: 'số'\nTarget:    'số'\nEpoch 3/25, Train Loss: 0.2046\nValidation CER: 11.48%, WER: 31.67%, ACC: 90.02%\nModel improved. Saving model to best_model.pth\nPredicted: 'mắt'\nTarget:    'mắt'\nPredicted: 'nác'\nTarget:    'rác'\nPredicted: 'cho'\nTarget:    'cho'\nPredicted: 'chật'\nTarget:    'chật'\nPredicted: 'nhậu'\nTarget:    'nhậu'\nEpoch 4/25, Train Loss: 0.1593\nValidation CER: 10.41%, WER: 29.42%, ACC: 91.29%\nModel improved. Saving model to best_model.pth\nPredicted: 'cấp'\nTarget:    'cấp'\nPredicted: 'thu'\nTarget:    'thu'\nPredicted: 'cùng'\nTarget:    'cũng'\nPredicted: 'cho'\nTarget:    'cho'\nPredicted: 'số'\nTarget:    'số'\nEpoch 5/25, Train Loss: 0.1295\nValidation CER: 10.06%, WER: 28.88%, ACC: 91.82%\nModel improved. Saving model to best_model.pth\nPredicted: 'Tất'\nTarget:    'Tất'\nPredicted: 'Hực'\nTarget:    'Huệ'\nPredicted: 'này'\nTarget:    'này'\nPredicted: 'thànhh'\nTarget:    'thành'\nPredicted: 'thưức'\nTarget:    'Phước'\nEpoch 6/25, Train Loss: 0.1102\nValidation CER: 9.38%, WER: 28.65%, ACC: 92.36%\nModel improved. Saving model to best_model.pth\nPredicted: 'vậy'\nTarget:    'vậy'\nPredicted: 'người'\nTarget:    'người'\nPredicted: 'ông'\nTarget:    'ông'\nPredicted: 'nối'\nTarget:    'nổi'\nPredicted: 'buết'\nTarget:    'hạnh'\nEpoch 7/25, Train Loss: 0.0947\nValidation CER: 8.66%, WER: 24.60%, ACC: 92.78%\nModel improved. Saving model to best_model.pth\nPredicted: 'ĐDVV'\nTarget:    'ĐDV'\nPredicted: 'sẽ'\nTarget:    'sẽ'\nPredicted: 'tiền'\nTarget:    'tiền'\nPredicted: 'đườngg'\nTarget:    'đường'\nPredicted: 'phê'\nTarget:    'phê'\nEpoch 8/25, Train Loss: 0.0850\nValidation CER: 9.18%, WER: 27.12%, ACC: 92.63%\nEarlyStopping counter: 1 / 5\nPredicted: 'trên'\nTarget:    'trên'\nPredicted: 'phần'\nTarget:    'phần'\nPredicted: 'việt'\nTarget:    'Việt'\nPredicted: 'vùi'\nTarget:    'vui'\nPredicted: 'từ'\nTarget:    'từ'\nEpoch 9/25, Train Loss: 0.0743\nValidation CER: 8.75%, WER: 26.04%, ACC: 92.99%\nModel improved. Saving model to best_model.pth\nPredicted: 'cứu'\nTarget:    'cứu'\nPredicted: 'khổ'\nTarget:    'khổ'\nPredicted: 'Sau'\nTarget:    'Sau'\nPredicted: 'HCM'\nTarget:    'HCM'\nPredicted: 'nhất'\nTarget:    'nhất'\nEpoch 10/25, Train Loss: 0.0672\nValidation CER: 8.92%, WER: 26.03%, ACC: 92.83%\nEarlyStopping counter: 1 / 5\nPredicted: 'được'\nTarget:    'được'\nPredicted: 'một'\nTarget:    'một'\nPredicted: 'nói'\nTarget:    'nói'\nPredicted: 'nhón'\nTarget:    'nhím'\nPredicted: 'quyến'\nTarget:    'quyết'\nEpoch 11/25, Train Loss: 0.0627\nValidation CER: 8.95%, WER: 26.19%, ACC: 92.71%\nEarlyStopping counter: 2 / 5\nPredicted: 'qua'\nTarget:    'qua'\nPredicted: 'sao'\nTarget:    'sao'\nPredicted: 'gấn'\nTarget:    'gấu'\nPredicted: 'tính'\nTarget:    'tính'\nPredicted: 'ngangg'\nTarget:    'ngang'\nEpoch 12/25, Train Loss: 0.0586\nValidation CER: 10.05%, WER: 28.24%, ACC: 92.43%\nEarlyStopping counter: 3 / 5\nPredicted: 'ngày'\nTarget:    'ngày'\nPredicted: 'Nghĩaa'\nTarget:    'Nghĩa'\nPredicted: 'đó'\nTarget:    'đó'\nPredicted: 'bênh'\nTarget:    'bệnh'\nPredicted: 'ứng'\nTarget:    'ứng'\nEpoch 13/25, Train Loss: 0.0524\nValidation CER: 8.88%, WER: 25.88%, ACC: 92.86%\nEarlyStopping counter: 4 / 5\nPredicted: 'ra'\nTarget:    'ra'\nPredicted: 'tờ'\nTarget:    'tờ'\nPredicted: 'suy'\nTarget:    'huy'\nPredicted: 'ở'\nTarget:    'ở'\nPredicted: 'ản'\nTarget:    'ảm'\nEpoch 14/25, Train Loss: 0.0499\nValidation CER: 8.10%, WER: 24.22%, ACC: 93.36%\nModel improved. Saving model to best_model.pth\nPredicted: 'Sở'\nTarget:    'Sở'\nPredicted: 'áu'\nTarget:    'án'\nPredicted: 'vẫn'\nTarget:    'vẫn'\nPredicted: 'Tuấn'\nTarget:    'Tuấn'\nPredicted: 'Bộ'\nTarget:    'Bộ'\nEpoch 15/25, Train Loss: 0.0463\nValidation CER: 9.20%, WER: 27.48%, ACC: 92.91%\nEarlyStopping counter: 1 / 5\nPredicted: 'phát'\nTarget:    'phát'\nPredicted: 'T'\nTarget:    'TP'\nPredicted: 'Và'\nTarget:    'Và'\nPredicted: '000'\nTarget:    '000'\nPredicted: 'quyếtt'\nTarget:    'quyết'\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#load saved model\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.to(device)\n\navg_cer, avg_wer, _ = evaluate(model, test_loader, vocab, device)\nprint(f\"Test Set - CER: {avg_cer * 100:.2f}%, WER: {avg_wer * 100:.2f}%\")","metadata":{"id":"bRrrhpLdS02v","trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 4))\n\n#train loss\nplt.subplot(1, 3, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss per Epoch')\nplt.grid(True)\n\n#val acc\nplt.subplot(1, 3, 2)\nplt.plot([acc * 100 for acc in val_accs], label='Validation ACC (%)', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy per Epoch')\nplt.grid(True)\n\n#val cer\nplt.subplot(1, 3, 3)\nplt.plot([cer * 100 for cer in val_cers], label='Validation CER (%)', color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('CER (%)')\nplt.title('Validation CER per Epoch')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"YxL_Biv_S2Kn","trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import torch.nn.functional as F\n# import numpy as np\n\n# def plot_pic(model, dataloader, vocab, device):\n#     model.eval()\n\n#     with torch.no_grad():\n#         for images, labels_padded, label_lengths in dataloader:\n#             images = images.to(device)\n#             labels_padded = labels_padded.to(device)\n\n#             outputs = model(images)  # (T, B, C)\n#             log_probs = F.log_softmax(outputs, dim=2)\n#             predicted_indices = log_probs.argmax(dim=2)  # (T, B)\n\n#             # Đổi thành (B, T) cho vocab decode\n#             predicted_texts = vocab.batch_decode(predicted_indices.permute(1, 0).cpu().numpy())\n\n#             labels_padded_np = labels_padded.cpu().numpy()\n#             label_lengths_np = label_lengths.cpu().numpy()\n#             target_texts = []\n#             for i in range(len(labels_padded_np)):\n#                 unpadded = labels_padded_np[i][:label_lengths_np[i]]\n#                 decoded = vocab.decode(unpadded)\n#                 target_texts.append(decoded)\n\n#             for i in range(len(predicted_texts)):\n#                 img = images[i].cpu().squeeze().numpy()\n#                 if img.max() <= 1.0:\n#                     img = (img * 255).astype(np.uint8)\n\n#                 plt.imshow(img, cmap='gray')\n#                 plt.title(f\"Predicted: {predicted_texts[i]}\\nTarget: {target_texts[i]}\")\n#                 plt.axis('off')\n#                 plt.show()\n\n#             break\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_pic(model, test_loader, vocab, device)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import torch.nn.functional as F\n# import numpy as np\n# import random\n\n# def plot_random_pics(model, dataloader, vocab, device, num_samples=7):\n#     model.eval()\n\n#     with torch.no_grad():\n#         for images, labels_padded, label_lengths in dataloader:\n#             images = images.to(device)\n#             labels_padded = labels_padded.to(device)\n\n#             outputs = model(images)  # (T, B, C)\n#             log_probs = F.log_softmax(outputs, dim=2)\n#             predicted_indices = log_probs.argmax(dim=2)  # (T, B)\n\n#             predicted_texts = vocab.batch_decode(predicted_indices.permute(1, 0).cpu().numpy())\n\n#             labels_padded_np = labels_padded.cpu().numpy()\n#             label_lengths_np = label_lengths.cpu().numpy()\n#             target_texts = []\n#             for i in range(len(labels_padded_np)):\n#                 unpadded = labels_padded_np[i][:label_lengths_np[i]]\n#                 decoded = vocab.decode(unpadded)\n#                 target_texts.append(decoded)\n\n#             indices = random.sample(range(len(predicted_texts)), k=min(num_samples, len(predicted_texts)))\n\n#             for i in indices:\n#                 img = images[i].cpu().squeeze().numpy()\n#                 if img.max() <= 1.0:\n#                     img = (img * 255).astype(np.uint8)\n\n#                 plt.figure(figsize=(5, 3))\n#                 plt.imshow(img, cmap='gray')\n#                 plt.title(f\"Predicted: {predicted_texts[i]}\\nTarget: {target_texts[i]}\")\n#                 plt.axis('off')\n#                 plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\ndef predict_and_show_batch(model, dataloader, vocab, device, n=5):\n    model.eval()\n    shown = 0\n\n    with torch.no_grad():\n        for images, labels_padded, label_lengths in dataloader:\n            images = images.to(device)\n            labels_padded = labels_padded.to(device)\n\n            # Forward\n            outputs = model(images)  # (seq_len, batch, vocab_size)\n            log_probs = F.log_softmax(outputs, dim=2)\n            predicted_indices = log_probs.argmax(dim=2)  # (seq_len, batch)\n\n            # Decode predicted and true labels\n            predicted_texts = vocab.batch_decode(predicted_indices.T.cpu().numpy())\n\n            labels_padded_np = labels_padded.cpu().numpy()\n            label_lengths_np = label_lengths.cpu().numpy()\n            target_texts = []\n            for i in range(len(labels_padded_np)):\n                unpadded = labels_padded_np[i][:label_lengths_np[i]]\n                decoded = vocab.decode(unpadded)\n                target_texts.append(decoded)\n\n            # Hiển thị ảnh + nhãn dự đoán\n            for i in range(len(images)):\n                img = images[i].cpu().squeeze().numpy()  # (1, H, W) -> (H, W)\n                if img.max() <= 1.0:\n                    img = (img * 255).astype('uint8')\n\n                pred = predicted_texts[i].replace('*', '')\n                target = target_texts[i]\n\n                plt.imshow(img, cmap='gray')\n                plt.title(f\" Predicted: {pred}\\n Ground Truth: {target}\")\n                plt.axis('off')\n                plt.show()\n\n                shown += 1\n                if shown >= n:\n                    return  # kết thúc sau khi hiển thị đủ n ảnh\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_and_show_batch(model, test_loader, vocab, device, n=5)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef predict_and_show_single(df, image_dir, index, model, vocab, device, img_h=64, char_w=32):\n    model.eval()\n\n    # Lấy ID và label\n    img_id = df.iloc[index]['id']\n    label_text = df.iloc[index]['label']\n    label_len = len(label_text)\n\n    # Đọc và xử lý ảnh như trong `preprocess_image`\n    image_path = f\"{image_dir}/{img_id}.png\"\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    if img is None:\n        print(f\"❌ Không thể đọc ảnh {image_path}\")\n        return\n\n    # Tiền xử lý\n    image = cv2.GaussianBlur(img, (5, 5), 0)\n    image = cv2.adaptiveThreshold(\n        image, 255, \n        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        cv2.THRESH_BINARY, 11, 5\n    )\n    kernel = np.ones((11, 11), np.uint8)\n    image = cv2.erode(image, kernel, iterations=1)\n\n    # Resize ảnh theo chiều cao chuẩn\n    cur_h, cur_w = image.shape\n    modified_w = int(cur_w * (img_h / cur_h))\n    img_w = label_len * char_w\n    image = cv2.resize(image, (img_w, img_h), interpolation=cv2.INTER_AREA)\n\n    # Convert sang tensor\n    image_tensor = torch.tensor(image / 255.0, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n\n    # Dự đoán\n    with torch.no_grad():\n        output = model(image_tensor)  # (T, B=1, C)\n        log_probs = F.log_softmax(output, dim=2)\n        pred_indices = log_probs.argmax(dim=2).squeeze(1).cpu().numpy().tolist()\n        pred_text = vocab.decode(pred_indices).replace('*', '')\n\n    # Hiển thị kết quả\n    plt.imshow(img, cmap='gray')\n    plt.title(f\"Predicted: {pred_text}\\n Label: {label_text}\")\n    plt.axis('off')\n    plt.show()\n\n    return pred_text\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_and_show_single(\n    df=test_word_data,\n    image_dir='/kaggle/input/Vietnam/test_word',\n    index=41,\n    model=model,\n    vocab=vocab,\n    device=device\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_and_show_batch(model, dataloader, dataset, vocab, device, n=5):\n    model.eval()\n    shown = 0\n    start_idx = 0 \n\n    with torch.no_grad():\n        for batch_idx, (images, labels_padded, label_lengths) in enumerate(dataloader):\n            batch_size = images.size(0)\n            images = images.to(device)\n            labels_padded = labels_padded.to(device)\n\n            outputs = model(images)  # my lovely note: đừng xóa shape, xóa xong bị khùn á: (seq_len, batch, vocab_size)\n            log_probs = F.log_softmax(outputs, dim=2)\n            predicted_indices = log_probs.argmax(dim=2)  # (seq_len, batch)\n\n            predicted_texts = vocab.batch_decode(predicted_indices.T.cpu().numpy())\n            labels_padded_np = labels_padded.cpu().numpy()\n            label_lengths_np = label_lengths.cpu().numpy()\n\n            target_texts = []\n            for i in range(len(labels_padded_np)):\n                unpadded = labels_padded_np[i][:label_lengths_np[i]]\n                decoded = vocab.decode(unpadded)\n                target_texts.append(decoded)\n\n            for i in range(batch_size):\n                idx_in_dataset = start_idx + i\n                img_id = dataset.data.iloc[idx_in_dataset]['id']\n\n                img = images[i].cpu().squeeze().numpy()\n                if img.max() <= 1.0:\n                    img = (img * 255).astype('uint8')\n\n                pred = predicted_texts[i].replace('*', '')\n                target = target_texts[i]\n                # nhớ in id ảnh cho Nh\n                plt.imshow(img, cmap='gray')\n                plt.title(f\" ID: {img_id}\\n Predicted: {pred}\\n Ground Truth: {target}\")\n                plt.axis('off')\n                plt.show()\n\n                shown += 1\n                if shown >= n:\n                    return\n\n            start_idx += batch_size\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_and_show_batch(model, test_loader, word_test_dataset, vocab, device, n=5)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T15:57:51.402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}